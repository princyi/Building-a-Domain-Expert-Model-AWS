Pre-trained Model Evaluation
Criteria	Submission Requirements
Deploy the Llama2 Model on AWS Sagemaker

The Llama2 model is successfully deployed on AWS Sagemaker. The output of the Model_Evaluation.ipynb file verifies deployment Screenshots in the report show the model deployed in the SageMaker environment

Evaluate the Pre-trained Llama2 Text Generation Large Language Model for Domain Knowledge

Model evaluation uses input to evaluate generation of domain-specific content, based on chosen domain for fine-tuning. The model_evaluation.ipynb notebook contains relevant examples and output cells. The response of the model to domain-specific inputs is documented in the project report.

Fine-tuning a Large Language Model
Criteria	Submission Requirements
Fine-tune a Large Language Model with a Domain-Specific Dataset

The model is fine-tuned using a dataset relevant to the chosen domain (Financial, Healthcare, IT). The model_finetuning.ipynb notebook demonstrates the fine-tuning process The dataset used for fine-tuning is set to the chosen domain in the Model_FineTuning.ipynb notebook file. Screenshots of the fine-tuning cell output are present.

Evaluate the Fine-tuned Llama2 Large Language Model
Criteria	Submission Requirements
Deploy the Fine-tuned Llama2 Model on AWS Sagemaker

The fine-tuned Llama2 model is successfully deployed on AWS Sagemaker. Deployment steps are shown in the Model_FineTuning.ipynb notebook cell output Screenshots show the deployed model in the SageMaker environment.

Evaluate the Fine-tuned Llama2 Text Generation Large Language Model on Text Generation Tasks and Domain Knowledge

Evaluation of the fine-tuned model's performance on domain-specific text generation tasks by providing domain-specific input to the fine-tuned model. The model_finetuning.ipynb notebook includes examples of the model's output post-fine-tuning. Documentation of the improvements or changes in the model's performance after fine-tuning is provided.in the Project Documentation Report.
